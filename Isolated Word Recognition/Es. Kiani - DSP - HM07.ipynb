{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd11359-d2aa-44f4-96ca-6cd36819d73e",
   "metadata": {},
   "source": [
    "# Esfandiar Kiani (40311614) - DSP - HM07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f90c0-c129-4579-9424-7fec8294fa45",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8ae60c-09d0-46fc-9ae9-4bf5fa878cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from dtw import dtw\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, HubertModel\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import WAV2VEC2_BASE\n",
    "from torchaudio.pipelines import HUBERT_BASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abec8b-7234-4d33-9c67-2f12868dcdfc",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee326570-6b36-418c-87f1-4aa4539b43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"D:/M.A/T1/DSP/Assignments/HM07/Dataset/Train\"\n",
    "TEST_PATH  = \"D:/M.A/T1/DSP/Assignments/HM07/Dataset/Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ae744-02bb-4894-baf8-17f29ed89834",
   "metadata": {},
   "source": [
    "## Feature extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0bb401-3601-4094-8659-7462cb9e02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(filepath, method):\n",
    "    y, sr = librosa.load(filepath, sr=16000)\n",
    "    \n",
    "    if method == 'cepstrum':\n",
    "        spectrum = np.fft.fft(y)\n",
    "        log_spectrum = np.log(np.abs(spectrum) + 1e-10)\n",
    "        cepstrum = np.fft.ifft(log_spectrum).real[:12]\n",
    "        return cepstrum\n",
    "        \n",
    "    elif method == 'mfcc':\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
    "        return np.mean(mfccs, axis=1)\n",
    "        \n",
    "    elif method == 'mfcc_energy':\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
    "        energy = np.mean(librosa.feature.rms(y=y))\n",
    "        return np.append(np.mean(mfccs, axis=1), energy)\n",
    "        \n",
    "    elif method == 'mfcc_delta':\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
    "        delta = librosa.feature.delta(mfccs)\n",
    "        energy = np.mean(librosa.feature.rms(y=y))\n",
    "        return np.append(np.append(np.mean(mfccs, axis=1), energy), np.mean(delta, axis=1))\n",
    "        \n",
    "    elif method == 'mfcc_delta2':\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
    "        delta = librosa.feature.delta(mfccs)\n",
    "        delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "        energy = np.mean(librosa.feature.rms(y=y))\n",
    "        return np.concatenate([np.mean(mfccs, axis=1), [energy], np.mean(delta, axis=1), np.mean(delta2, axis=1)])\n",
    "        \n",
    "    elif method == 'wav2vec2':\n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        inputs = processor(y, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n",
    "        \n",
    "    elif method == \"hubert\":\n",
    "        bundle = HUBERT_BASE\n",
    "        model = bundle.get_model()\n",
    "\n",
    "        waveform = torchaudio.functional.resample(y, sr, bundle.sample_rate)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            features, _ = model(waveform)\n",
    "\n",
    "        return features.mean(dim=1).detach().numpy().flatten()\n",
    "\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c28da-2d25-4e90-b826-c5e3b4334d0d",
   "metadata": {},
   "source": [
    "## Methods & Result saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be1cb59-e7f2-4850-8614-a07c1362b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['cepstrum', 'mfcc', 'mfcc_energy', 'mfcc_delta', 'mfcc_delta2', 'wav2vec2', 'hubert']\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1cdea-9697-45e5-bf4e-95c6f0aeb40a",
   "metadata": {},
   "source": [
    "## Train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd675a1-9faf-4c13-9447-c9653b09c193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c18471a10b842768095319237f8c6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0569c620d3245a6acd1af0058c6e305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaefac5919424468aa5d4bbe72c2a3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e988fe40fc4a16910f7521dfe73105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8473d3f6495434586c4f713f2e3f001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32702428d4884b1fa152e94807ac99a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c804b59c573d43c2ad97c5268ff4539a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299b736a878a48e18ec23e7fce4cebdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:61: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'facebook/hubert-base-ls960'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/hubert-base-ls960' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:59\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\processing_utils.py:974\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    972\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m--> 974\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\processing_utils.py:1020\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1018\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m-> 1020\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:921\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    919\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m         )\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'facebook/hubert-base-ls960'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/hubert-base-ls960' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m test_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     ref_feat \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mTRAIN_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     test_feat \u001b[38;5;241m=\u001b[39m extract_features(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, method)\n\u001b[0;32m      9\u001b[0m     reference_features\u001b[38;5;241m.\u001b[39mappend(ref_feat)\n",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(filepath, method)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhubert\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     processor \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2Processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/hubert-base-ls960\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     model \u001b[38;5;241m=\u001b[39m HubertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/hubert-base-ls960\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(y, sampling_rate\u001b[38;5;241m=\u001b[39msr, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:71\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a tokenizer inside \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from a config that does not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m include a `tokenizer_class` attribute is deprecated and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     70\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m Wav2Vec2FeatureExtractor\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 71\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2CTCTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(feature_extractor\u001b[38;5;241m=\u001b[39mfeature_extractor, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'facebook/hubert-base-ls960'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/hubert-base-ls960' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "for method in methods:\n",
    "    reference_features = []\n",
    "    test_features = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        ref_feat = extract_features(f\"{TRAIN_PATH}/{i}.wav\", method)\n",
    "        test_feat = extract_features(f\"{TEST_PATH}/{i}.wav\", method)\n",
    "        reference_features.append(ref_feat)\n",
    "        test_features.append(test_feat)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    for i, test in enumerate(test_features):\n",
    "        distances = [dtw(test, ref).distance for ref in reference_features]\n",
    "        if np.argmin(distances) == i:\n",
    "            correct += 1\n",
    "            \n",
    "    accuracy = correct / 10 * 100\n",
    "    \n",
    "    results.append((method, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27edda3f-48ac-46aa-b895-be8bf003bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features2(filepath, method):\n",
    "    y, sr = librosa.load(filepath, sr=16000)\n",
    "    if method == \"hubert\":\n",
    "        bundle = HUBERT_BASE\n",
    "        model = bundle.get_model()\n",
    "\n",
    "        waveform = torchaudio.functional.resample(y, sr, bundle.sample_rate)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            features, _ = model(waveform)\n",
    "\n",
    "        return features.mean(dim=1).detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c70912-1c01-4cff-b68c-6694584b95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_features = []\n",
    "test_features = []\n",
    "method = 'hubert'\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    ref_feat  = extract_features2(f\"{TRAIN_PATH}/{i}.wav\", method)\n",
    "    test_feat = extract_features2(f\"{TEST_PATH}/{i}.wav\", method)\n",
    "    reference_features.append(ref_feat)\n",
    "    test_features.append(test_feat)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i, test in enumerate(test_features):\n",
    "    distances = [dtw(test, ref).distance for ref in reference_features]\n",
    "    if np.argmin(distances) == i:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / 10 * 100\n",
    "\n",
    "results.append((method, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4250860-ee25-4031-bc5f-653f9df7b3b2",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa34233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=[\"Method\", \"Accuracy\"])\n",
    "df.to_csv(\"recognition_results.csv\", index=False)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
